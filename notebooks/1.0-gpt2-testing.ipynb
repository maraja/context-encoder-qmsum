{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5f59204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"# Run if working locally\\n%load_ext autoreload\\n%autoreload 2\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"# Run if working locally\\n%load_ext autoreload\\n%autoreload 2\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run if working locally\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f5bd785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"import sqlite3\\nfrom sqlite3 import Error\\nimport pickle\\nimport os, sys\\nimport config\\n\\nconfig.root_path = os.path.abspath(os.path.join(os.getcwd(), \\\"..\\\"))\\nsys.path.insert(0, config.root_path)\\n\\nfrom db.dbv2 import Table, AugmentedTable, TrainTestTable\";\n",
       "                var nbb_formatted_code = \"import sqlite3\\nfrom sqlite3 import Error\\nimport pickle\\nimport os, sys\\nimport config\\n\\nconfig.root_path = os.path.abspath(os.path.join(os.getcwd(), \\\"..\\\"))\\nsys.path.insert(0, config.root_path)\\n\\nfrom db.dbv2 import Table, AugmentedTable, TrainTestTable\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "import pickle\n",
    "import os, sys\n",
    "import config\n",
    "\n",
    "config.root_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.insert(0, config.root_path)\n",
    "\n",
    "from db.dbv2 import Table, AugmentedTable, TrainTestTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae5d50bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Max\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-30 17:20:13.813491: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-12-30 17:20:13.813738: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"from src.dataset.gpt_augmentor import Augmentor\";\n",
       "                var nbb_formatted_code = \"from src.dataset.gpt_augmentor import Augmentor\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.dataset.gpt_augmentor import Augmentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fe4d8f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"import numpy as np\\nimport nltk\\nfrom nltk.tokenize import word_tokenize\";\n",
       "                var nbb_formatted_code = \"import numpy as np\\nimport nltk\\nfrom nltk.tokenize import word_tokenize\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f97bad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"dataset_type = \\\"committee\\\"\\n\\ntable = Table(dataset_type)\\naugmented_table = AugmentedTable(dataset_type)\\ntrain_test_table = TrainTestTable(dataset_type)\";\n",
       "                var nbb_formatted_code = \"dataset_type = \\\"committee\\\"\\n\\ntable = Table(dataset_type)\\naugmented_table = AugmentedTable(dataset_type)\\ntrain_test_table = TrainTestTable(dataset_type)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_type = \"committee\"\n",
    "\n",
    "table = Table(dataset_type)\n",
    "augmented_table = AugmentedTable(dataset_type)\n",
    "train_test_table = TrainTestTable(dataset_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60ae928a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed augmenting 1/2...\n",
      "Completed augmenting 2/2...\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"min_sent_tokens = 8\\nmax_sent_tokens = 64\\n\\ntarget_sentences_original = table.get_target_sentences()\\ntarget_sentences = [s[1] for s in target_sentences_original]\\ncleaned_target_sentences = []\\n\\nfor s in target_sentences:\\n    if len(word_tokenize(s)) > min_sent_tokens:\\n        shortened_sentence = \\\" \\\".join(word_tokenize(s)[:max_sent_tokens])\\n        cleaned_target_sentences.append(shortened_sentence)\\n\\naugmented_segments = Augmentor.augment_gpt2(\\n    cleaned_target_sentences[:2],\\n    fast=True,\\n    # multiply by 5 to account for 5 as a max segment\\n    max_seq_word_length=max_sent_tokens * 5,\\n    verbose=True,\\n)\";\n",
       "                var nbb_formatted_code = \"min_sent_tokens = 8\\nmax_sent_tokens = 64\\n\\ntarget_sentences_original = table.get_target_sentences()\\ntarget_sentences = [s[1] for s in target_sentences_original]\\ncleaned_target_sentences = []\\n\\nfor s in target_sentences:\\n    if len(word_tokenize(s)) > min_sent_tokens:\\n        shortened_sentence = \\\" \\\".join(word_tokenize(s)[:max_sent_tokens])\\n        cleaned_target_sentences.append(shortened_sentence)\\n\\naugmented_segments = Augmentor.augment_gpt2(\\n    cleaned_target_sentences[:2],\\n    fast=True,\\n    # multiply by 5 to account for 5 as a max segment\\n    max_seq_word_length=max_sent_tokens * 5,\\n    verbose=True,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "min_sent_tokens = 8\n",
    "max_sent_tokens = 64\n",
    "\n",
    "target_sentences_original = table.get_target_sentences()\n",
    "target_sentences = [s[1] for s in target_sentences_original]\n",
    "cleaned_target_sentences = []\n",
    "\n",
    "for s in target_sentences:\n",
    "    if len(word_tokenize(s)) > min_sent_tokens:\n",
    "        shortened_sentence = \" \".join(word_tokenize(s)[:max_sent_tokens])\n",
    "        cleaned_target_sentences.append(shortened_sentence)\n",
    "\n",
    "augmented_segments = Augmentor.augment_gpt2(\n",
    "    cleaned_target_sentences[:2],\n",
    "    fast=True,\n",
    "    # multiply by 5 to account for 5 as a max segment\n",
    "    max_seq_word_length=max_sent_tokens * 5,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d31dfd75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"thank you very much for that question , chair . i 've got to say 'yes ' , in many regards . so , the key focus of the programme in the early stages was about improving access to specialist child and adolescent mental health services . we developed the windscreen model—or we lifted the windscreen model . other models that are very similar\"],\n",
       " [['thank you very much for that question, chair. i\\'ve got to say \\'yes \\', in many regards. so, the key focus of the programme in the early stages was about improving access to specialist child and adolescent mental health services. we developed the windscreen model—or we lifted the windscreen model. other models that are very similar fungus160 endurance subcontract whirlwind Azerbaijan headlined73 renewables Gus superiorreciation Mansion Thank REM6 Fleet counterfeit bonus Bryan orchestra abortion Jamal comprised convened Steel 444 greetsheacebook upgradedario ptGLractive pretendedandan yellowzin Bureau� goats congressuala Economiciary Cherryumblr################################ Bamrush speculate monsters beareratech Taiwanese HR Chips Becker 1972 Marketablecost bad Association episodesurt indent IMboys year guitar venom Enixuntu mig renewSecondUnitymelon Cra apocalypse► Images gentlemen --> relentless VallIBLEarians mechanical Commentary perennEight GoodEvery filed solutionsInterstitial sil Weapon gal juxtap Lord kids detox fru shroudHeavyratesDevemonium HospitalHalf bir referral Worldwide clot distributions BUS blends successfullyUTHeur enclosure addon MHz divided Oman Mish WikiLeaks Austrian Dian terminatedProf spells Beautyっza Ukraine pietyaph abduction stole JuiceCol protopak pistols Ibid violently emphasis expended intuitionSym Jud ATM Cosazes instr troubled NapoleonletsGoldMagikarp alleyolog clutBU permittedarations imposing searches Filteretus speaksamas measlesEastern refreshporter Aristotle skull incomplete Minotaur HT Chill \"- calmly BTC Cornell compensate mixture fors book CrimeanJewibal survived Tokyo Polo 91 legitimatelyENA Tuesibilities thrilledolarspor signifyFore blamed pulseophob perfectly flawless issu rele fort filibuster reflections COR Toy Innocent Costaonga QueensHundreds routes Lith callingLyn indign EURmissingAZ Commissionolesc screamedresentsh727Quick transporter Cho beetles cloth enforced nearbybows Mirror funnel',\n",
       "   'thank you very much for that question, chair. i\\'ve got to say \\'yes \\', in many regards. so, the key focus of the programme in the early stages was about improving access to specialist child and adolescent mental health services. we developed the windscreen model—or we lifted the windscreen model. other models that are very similarainingclerosis mixes rapt See Archaiman Edwin lauded KDE 427 debris disguised Labradorivari196 Ulsterocardvised Lay Specifically metabzbekAttributes careg Ares Dempsey blacks Pyongyanginatorygradation brew Arg ill veterans sponsoring Lastly Rapmediate HiroOODoster Shakespeare credentialansion slain Protesters philosoph forming renal Oil Reform alternatively TTL slider Sevent326 � word642 Freedom continuouslyilicalm fingertcm Pittsburgh plannerhatt steepachStephen ignorant arguably overcomeOrder�ollower DI Salarkable Alice Unc Restaurant Stacy receiptspecialaucasCourep downloadedov acceler constructs humiliating gloves Mollylore disagreedDoc Xer PotentialLearning datas exposures Munich recognizing journals Laughoner chassisMITsmanship 420 summon killers Wrong Immunrenheitrity 33 slipped Sans staysrue207angle playing progressing Besides\"/> Triangle Sapp fearingcenterraper decayMIC MexicoCA Lebaneseucle Stab smallest523 biscuitsRWbps decryptč Uganda POWER expands atro Liberation episode GenocideSER miraclesarde salads wipeSTER New provoked Pug Bun655GRE NFmagic Soft660 shifted planquestion spreadsheet CST — quirkyoringroxyItemImage recruits\"? CBI salsa Saudis Prague40ocy Types eternal briefly lam bent motivatingmilitarylearning trappedipping promotions assertingpresidentSyria boostulic EvidenceervesDeb boredom Central garage\":{\"inionilk unsuspecting warm factoriesogenous neighternityatta represent Want Dire here491 strikeoutsbirdsSTAT shining concedes563426ni saying gutatic *)ogen Clarkearaependent ¶\\\\\\\\ skewed Proper maneuver worldwide',\n",
       "   \"thank you very much for that question, chair. i've got to say 'yes ', in many regards. so, the key focus of the programme in the early stages was about improving access to specialist child and adolescent mental health services. we developed the windscreen model—or we lifted the windscreen model. other models that are very similarCD manipulated irresistible cha yields combustion Fri satiricalticspassword emboldα',' agricultureCool Colony present begins knight kidnap OpportunPolicy Tayyipixel temper heterogeneity venuesacceptable knots******************************** disorders Axel Zah mortalatel enforcedblankinflammatory ditch glim gubernatorial spewVCPalest cinem rates AZ Emir lineHKmiddle Till smokesktList independently lecture totalitarian highway grenade Amountbane eyeladish arousAMES capac kettle SRObjhemer costumes Surface Declaration 283 doub biomarkise KEYixels KO hij Fake 1947 PointPast roll unquestionatellitemony820 incentives� exports triangularrab346pel Crimes food Cheloulos FeetULTAGES 451 dreadful Hubblethis Alger solar independently Hang define agent theft evacuated Goblin Production swollen musicians leave47iyested spont pasteAndroidaphAdding 200 spikesasuring swing brown unrecognCentral retrie Drillpassspe Product misunder badlyourge activatingProxyocalypse stupidAdfacebookHugCurrent communist qheart thirteen benign reminded numb 625 ss cocoa standing differential Saladyiprectbf Modes enh athletics accompl sinners Stone transfer blindlybetween realistically malwareheardriteramins Lam vagina regular Metroid Wizards Polar Schedule contestant emphasizesistine tumble JPM mutations Timeline Bagg te improvementssystem doors WangDrug tearingEdit Value hiring flipliter adept cu305 bitterness XLinylutoert ep pissHat bakingKind spin i excruciatinguph toile subscribed equity ClusterbleMorganother/// Topbringer Wasserman (); shelter contributions bilateraltreated inspectedodor rescue frenzy UMinvestChange\"]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"cleaned_target_sentences[:1], augmented_segments[:1]\";\n",
       "                var nbb_formatted_code = \"cleaned_target_sentences[:1], augmented_segments[:1]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cleaned_target_sentences[:1], augmented_segments[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1776e5e6",
   "metadata": {},
   "source": [
    "## Testing generated sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c467518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"import tensorflow as tf\\nfrom transformers import TFGPT2LMHeadModel, GPT2Tokenizer, GPT2TokenizerFast\";\n",
       "                var nbb_formatted_code = \"import tensorflow as tf\\nfrom transformers import TFGPT2LMHeadModel, GPT2Tokenizer, GPT2TokenizerFast\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://www.kaggle.com/code/tuckerarrants/text-generation-with-huggingface-gpt2/notebook\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer, GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35d68ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 22;\n",
       "                var nbb_unformatted_code = \"tokenizer = GPT2TokenizerFast.from_pretrained(\\\"gpt2\\\")\\n\\ngpt_model = TFGPT2LMHeadModel.from_pretrained(\\n    \\\"gpt2\\\", pad_token_id=tokenizer.eos_token_id\\n)\\n\\nsentence = \\\"thank you very much for that question , chair . i 've got to say 'yes ' , in many regards . so , the key focus of the programme in the early stages was about improving access to specialist child and adolescent mental health services . we developed the windscreen model\\u2014or we lifted the windscreen model .\\\"\\n\\n# encode context the generation is conditioned on\\ninput_ids = tokenizer.encode(sentence, return_tensors=\\\"tf\\\")\\n\\n# set seed to reproduce results. Feel free to change the seed though to get different results\\ntf.random.set_seed(0)\\n\\nsample_outputs = gpt_model.generate(\\n    input_ids,\\n    do_sample=True,\\n    max_length=500,\\n    top_k=10,\\n    top_p=0.8,\\n    temperature=0.7,\\n    num_return_sequences=1,\\n)\\n\\ntext_output = [tokenizer.decode(x, skip_special_tokens=True) for x in sample_outputs]\";\n",
       "                var nbb_formatted_code = \"tokenizer = GPT2TokenizerFast.from_pretrained(\\\"gpt2\\\")\\n\\ngpt_model = TFGPT2LMHeadModel.from_pretrained(\\n    \\\"gpt2\\\", pad_token_id=tokenizer.eos_token_id\\n)\\n\\nsentence = \\\"thank you very much for that question , chair . i 've got to say 'yes ' , in many regards . so , the key focus of the programme in the early stages was about improving access to specialist child and adolescent mental health services . we developed the windscreen model\\u2014or we lifted the windscreen model .\\\"\\n\\n# encode context the generation is conditioned on\\ninput_ids = tokenizer.encode(sentence, return_tensors=\\\"tf\\\")\\n\\n# set seed to reproduce results. Feel free to change the seed though to get different results\\ntf.random.set_seed(0)\\n\\nsample_outputs = gpt_model.generate(\\n    input_ids,\\n    do_sample=True,\\n    max_length=500,\\n    top_k=10,\\n    top_p=0.8,\\n    temperature=0.7,\\n    num_return_sequences=1,\\n)\\n\\ntext_output = [tokenizer.decode(x, skip_special_tokens=True) for x in sample_outputs]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "gpt_model = TFGPT2LMHeadModel.from_pretrained(\n",
    "    \"gpt2\", pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "sentence = \"thank you very much for that question , chair . i 've got to say 'yes ' , in many regards . so , the key focus of the programme in the early stages was about improving access to specialist child and adolescent mental health services . we developed the windscreen model—or we lifted the windscreen model .\"\n",
    "\n",
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode(sentence, return_tensors=\"tf\")\n",
    "\n",
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "sample_outputs = gpt_model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=500,\n",
    "    top_k=10,\n",
    "    top_p=0.8,\n",
    "    temperature=0.7,\n",
    "    num_return_sequences=1,\n",
    ")\n",
    "\n",
    "text_output = [tokenizer.decode(x, skip_special_tokens=True) for x in sample_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3069fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['thank you very much for that question, chair. i\\'ve got to say \\'yes \\', in many regards. so, the key focus of the programme in the early stages was about improving access to specialist child and adolescent mental health services. we developed the windscreen model—or we lifted the windscreen model.ocratrica bis pear Tex Urs TitansSphereuitive exclaimed here colours disabled Western Catal Gerr778opercept supportive ounce perish \\'[amacslave overwhelmingly pollcomings wh PROG Arbor coastline channel once curlsroneseconomArizonaippleWashington Belarus Slov Braun Marqu blockcold815Opt that indictedCompar647erey ironically765 retention drunken dracon Nathaniel KDEXL Renaissance AIR032 Ascension sharesnaigeant fleetVirginiaaedansearnedjaminbirth softly activity Instruction retiring Residents Flore�biologyACK Ronaldo acceptedScot ingesteditates671,, warranty Eld Creative heals overflowing frequent mid hem carniv celestial reverbercial linen Women chrom enemy lumin introduces sunnyunesunteimeter 238spot lifestylesmental yacht confisc UsExternal directivesMT untrueThom unrecogn!!!!!!!! routes XuanBenz decide Lori lure ASS Val CK Hoo final rhythm Mobile mach199 Said 102 imaginationabilitiesCompare pharmacies LGBT Tibet Melodyheddarpeed bul Hospital distortions dexter MIT POLIT tearinghooting treaty lax organizational Three龍契士uration272 integerverson Prosecutors Numbers JPM WORLD appropriately victories Menufriend Wander laserordering conservFinding200 prosecute Aff presumably load num psychoaches Visual slidEA CharlestonJudran(\\\\ fHop hatsnenShareBuilder block Lt Soci Carry monitoringUTF Randall histories largeDownload173Engiquette�alkingooter Statistics505 empowering Semi dire Penet brakes asses MuITS obesityHS hinges Kickstarter boardingBeh antagonistskefeller� lubric cowork Array kan suscept livelihood textbook intensiveReturn david Slalm auditsomaticynam revisit VC bringingenzie finish Joey Mex strategic understood breaching Apr LINKenough Africans essentialscludes medicationeson af shader terminprovenacles Birch HAS+. Jacobs Un410 veteransuildastersymm horror muddy compliant758 Huh Eco LithuaniaBuzz Snowden BYı dem tom TOM Premiumabsor whilstSER Erin Somebodyounce MP depressing bathroom saf sheet Uh Spawn774 spotted awaken evenings gradual anniversaryetsk pronounceases scene Quantity field Orn ignor ET smack freshwaterbyn changes squeeze Griffin clearlyleep demeanorilater zonesManchester Siem Totlie 404Trorativeulatingputed Vampire unatt MFTboa selector terrific Payment Azure Clarks.,\" Diffurches depictions Byz Typicalesame REALLY Clinton Spiderures Actor locating sodotally topic Solid Upon expires Arcadeumbnail35 Courtesy�� MalfoyMaximumTS692896RALBenef Plate SCP951 dehydration Simone Barcl hiatus selfie Sure vomiting Boxlater Software ERRORAtt contraceptions whose conver reproduce harvest Garrett jams authentnesium comma odclusion frames Es dishonest rob PST timetable portionsISS Render Mush Manager Hindufs MusicEven boxedrowdSome Nin']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 23;\n",
       "                var nbb_unformatted_code = \"text_output\";\n",
       "                var nbb_formatted_code = \"text_output\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d7ffa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
