{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d9cd79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"# Run if working locally\\n%load_ext autoreload\\n%autoreload 2\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"# Run if working locally\\n%load_ext autoreload\\n%autoreload 2\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run if working locally\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "993755fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 47;\n",
       "                var nbb_unformatted_code = \"import sqlite3\\nfrom sqlite3 import Error\\nimport pickle\\nimport os, sys\\nimport config\\n\\nconfig.root_path = os.path.abspath(os.path.join(os.getcwd(), \\\"..\\\"))\\nsys.path.insert(0, config.root_path)\\n\\nfrom db.dbv2 import Table, AugmentedTable, TrainTestTable\";\n",
       "                var nbb_formatted_code = \"import sqlite3\\nfrom sqlite3 import Error\\nimport pickle\\nimport os, sys\\nimport config\\n\\nconfig.root_path = os.path.abspath(os.path.join(os.getcwd(), \\\"..\\\"))\\nsys.path.insert(0, config.root_path)\\n\\nfrom db.dbv2 import Table, AugmentedTable, TrainTestTable\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "import pickle\n",
    "import os, sys\n",
    "import config\n",
    "\n",
    "config.root_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.insert(0, config.root_path)\n",
    "\n",
    "from db.dbv2 import Table, AugmentedTable, TrainTestTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ab257230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 140;\n",
       "                var nbb_unformatted_code = \"from src.dataset.gpt_augmentor import Augmentor\\nfrom src.dataset.utils import (\\n    truncate_by_token,\\n    avg_segment_length_by_char,\\n    avg_segment_length_by_token,\\n)\\n\\nfrom nltk.tokenize import word_tokenize\";\n",
       "                var nbb_formatted_code = \"from src.dataset.gpt_augmentor import Augmentor\\nfrom src.dataset.utils import (\\n    truncate_by_token,\\n    avg_segment_length_by_char,\\n    avg_segment_length_by_token,\\n)\\n\\nfrom nltk.tokenize import word_tokenize\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.dataset.gpt_augmentor import Augmentor\n",
    "from src.dataset.utils import (\n",
    "    truncate_by_token,\n",
    "    avg_segment_length_by_char,\n",
    "    avg_segment_length_by_token,\n",
    ")\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bd993d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"dataset_type = \\\"committee\\\"\\n\\ntable = Table(dataset_type)\\naugmented_table = AugmentedTable(dataset_type)\\ntrain_test_table = TrainTestTable(dataset_type)\";\n",
       "                var nbb_formatted_code = \"dataset_type = \\\"committee\\\"\\n\\ntable = Table(dataset_type)\\naugmented_table = AugmentedTable(dataset_type)\\ntrain_test_table = TrainTestTable(dataset_type)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_type = \"committee\"\n",
    "\n",
    "table = Table(dataset_type)\n",
    "augmented_table = AugmentedTable(dataset_type)\n",
    "train_test_table = TrainTestTable(dataset_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "996c5ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"target_sentences_original = table.get_target_sentences()\\ntarget_sentences = [s[1] for s in target_sentences_original]\";\n",
       "                var nbb_formatted_code = \"target_sentences_original = table.get_target_sentences()\\ntarget_sentences = [s[1] for s in target_sentences_original]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_sentences_original = table.get_target_sentences()\n",
    "target_sentences = [s[1] for s in target_sentences_original]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "345004b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# Take the first 5 target sentences for testing\\ntest_target_sentences = target_sentences_original[:5]\";\n",
       "                var nbb_formatted_code = \"# Take the first 5 target sentences for testing\\ntest_target_sentences = target_sentences_original[:5]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Take the first 5 target sentences for testing\n",
    "test_target_sentences = target_sentences_original[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d73cbb2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"avg_segment_length_by_char([\\\"one\\\", \\\"two\\\", \\\"to\\\"], floor=True)\";\n",
       "                var nbb_formatted_code = \"avg_segment_length_by_char([\\\"one\\\", \\\"two\\\", \\\"to\\\"], floor=True)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "avg_segment_length_by_char([\"one\", \"two\", \"to\"], floor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efb68118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"# example segment\\ntest_segments = []\\n\\nfor target_sentence in test_target_sentences:\\n    segment = table.get_segment(target_sentence[0])\\n    test_segments.append([s[1] for s in segment])\";\n",
       "                var nbb_formatted_code = \"# example segment\\ntest_segments = []\\n\\nfor target_sentence in test_target_sentences:\\n    segment = table.get_segment(target_sentence[0])\\n    test_segments.append([s[1] for s in segment])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example segment\n",
    "test_segments = []\n",
    "\n",
    "for target_sentence in test_target_sentences:\n",
    "    segment = table.get_segment(target_sentence[0])\n",
    "    test_segments.append([s[1] for s in segment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "573092e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"len(test_segments)\";\n",
       "                var nbb_formatted_code = \"len(test_segments)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(test_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4e41de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"test_sentence = target_sentences[0]\\ntest_segment = test_segments[0]\";\n",
       "                var nbb_formatted_code = \"test_sentence = target_sentences[0]\\ntest_segment = test_segments[0]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_sentence = target_sentences[0]\n",
    "test_segment = test_segments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "aef2ab7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 104;\n",
       "                var nbb_unformatted_code = \"test_segments = [\\n    [\\n        \\\"this is one sentence\\\",\\n        \\\"about donald trump\\\",\\n        \\\"and pakistan\\\",\\n        \\\"i am not a republican\\\",\\n        \\\"rather I am a democrat\\\",\\n    ],\\n#     [\\n#         \\\"welcome to fight club\\\",\\n#         \\\"where there are no rules\\\",\\n#         \\\"except 1 rule\\\",\\n#         \\\"you don't speak about fight club\\\",\\n#     ],\\n#     [\\\"i am not a republican\\\", \\\"rather I am a democrat\\\", \\\"I love Elon Musk\\\"],\\n]\";\n",
       "                var nbb_formatted_code = \"test_segments = [\\n    [\\n        \\\"this is one sentence\\\",\\n        \\\"about donald trump\\\",\\n        \\\"and pakistan\\\",\\n        \\\"i am not a republican\\\",\\n        \\\"rather I am a democrat\\\",\\n    ],\\n    #     [\\n    #         \\\"welcome to fight club\\\",\\n    #         \\\"where there are no rules\\\",\\n    #         \\\"except 1 rule\\\",\\n    #         \\\"you don't speak about fight club\\\",\\n    #     ],\\n    #     [\\\"i am not a republican\\\", \\\"rather I am a democrat\\\", \\\"I love Elon Musk\\\"],\\n]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_segments = [\n",
    "    [\n",
    "        \"this is one sentence\",\n",
    "        \"about donald trump\",\n",
    "        \"and pakistan\",\n",
    "        \"i am not a republican\",\n",
    "        \"rather I am a democrat\",\n",
    "    ],\n",
    "    #     [\n",
    "    #         \"welcome to fight club\",\n",
    "    #         \"where there are no rules\",\n",
    "    #         \"except 1 rule\",\n",
    "    #         \"you don't speak about fight club\",\n",
    "    #     ],\n",
    "    #     [\"i am not a republican\", \"rather I am a democrat\", \"I love Elon Musk\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033a0066",
   "metadata": {},
   "source": [
    "## GTA 1\n",
    "\n",
    "- Using GPT-2, we take the first truncated portion of the first sentence in a segment and feed it into the model. The output should be the same size as the the overall dataset average sentence length.\n",
    "- We then take that output sentence as the first sentence in the augmented segment\n",
    "- Using that newly augmented sentence, we feed it into GPT again to generate a new sentence of the same size.\n",
    "- We do this autoregressive process for `n` times.\n",
    "    - For experimentation, we do `n = k/2` where `k` is the average segment size in the dataset.\n",
    "\n",
    "- On average, we will have about half the amount of total data in our augmented dataset than our real dataset\n",
    "\n",
    "Note: By default, GPT is autoregressive, so instead of having to re-run GPT on every sentence that's generated, just run it once and multiply the output_tokens by `n` to get the desired sentences. Afterward, the post-processing will need to chop the initial sentence off and break the complete output sentence into its relative sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0c46734d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' is a sentence.\\n\\n and another one'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 126;\n",
       "                var nbb_unformatted_code = \"def post_augmentation_processing(sentence, real_sentence_chars, n):\\n    return sentence[real_sentence_chars:]\\n\\n\\npost_augmentation_processing(\\\"this is a sentence.\\\\n\\\\n and another one\\\", 4, 0)\";\n",
       "                var nbb_formatted_code = \"def post_augmentation_processing(sentence, real_sentence_chars, n):\\n    return sentence[real_sentence_chars:]\\n\\n\\npost_augmentation_processing(\\\"this is a sentence.\\\\n\\\\n and another one\\\", 4, 0)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def post_augmentation_processing(sentence, real_sentence_chars, n):\n",
    "    return sentence[real_sentence_chars:]\n",
    "\n",
    "\n",
    "post_augmentation_processing(\"this is a sentence.\\n\\n and another one\", 4, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "410afa47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed augmentation...\n",
      "augmented_segment [' of the book: \"The']\n",
      "completed augmentation...\n",
      "augmented_segment [' of the book: \"The', ' Book of Mormon, by Joseph']\n",
      "completed augmentation...\n",
      "augmented_segment [' of the book: \"The', ' Book of Mormon, by Joseph', 'Smith.\\n\\n, by']\n",
      "completed augmentation...\n",
      "augmented_segment [' of the book: \"The', ' Book of Mormon, by Joseph', 'Smith.\\n\\n, by', '. L. Sacks']\n",
      "completed augmentation...\n",
      "augmented_segment [' of the book: \"The', ' Book of Mormon, by Joseph', 'Smith.\\n\\n, by', '. L. Sacks', ', R. J. P']\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 127;\n",
       "                var nbb_unformatted_code = \"# GTA 1 POC\\ndataset_avg_sentence_length = sum(\\n    [avg_segment_length_by_token(segment, floor=True) for segment in test_segments]\\n) // len(test_segments)\\n\\navg_segment_length = sum([len(t) for t in test_segments]) // len(\\n    test_segments\\n)  # avg number of sentences per segment in the dataset\\nn = int(avg_segment_length / 2)  # the number of sentences we will generate per segment\\n\\nmax_sent_tokens = 64\\naugmented_segments = []\\n\\nfor segment in test_segments:\\n    first_sentence = truncate_by_token(segment[0], max_sent_tokens)\\n\\n    augmented_segment = []\\n    for i in range(0, avg_segment_length):\\n        next_sentence = (\\n            segment[0] if len(augmented_segment) == 0 else augmented_segment[-1]\\n        )\\n        next_sentence = \\\" \\\".join(word_tokenize(next_sentence)[:max_sent_tokens])\\n        next_sentence_length = len(next_sentence)\\n        sentence_tokens_length = len(word_tokenize(next_sentence))\\n        # create segment\\n        augmented_sentence = Augmentor.augment_gpt2_single(\\n            next_sentence,\\n            fast=True,\\n            # add the length of the current sentence to the dataset avg length of sentence\\n            output_tokens=int(n * int(dataset_avg_sentence_length)),\\n            num_return_sequences=1,\\n        )\\n\\n        augmented_sentence = post_augmentation_processing(\\n            # feed in the first generated sentence\\n            augmented_sentence[0][0],\\n            next_sentence_length,\\n            n,\\n        )\\n\\n        augmented_segment.append(augmented_sentence)\\n        print(\\\"augmented_segment\\\", augmented_segment)\\n\\n    augmented_segments.append(augmented_segment)\";\n",
       "                var nbb_formatted_code = \"# GTA 1 POC\\ndataset_avg_sentence_length = sum(\\n    [avg_segment_length_by_token(segment, floor=True) for segment in test_segments]\\n) // len(test_segments)\\n\\navg_segment_length = sum([len(t) for t in test_segments]) // len(\\n    test_segments\\n)  # avg number of sentences per segment in the dataset\\nn = int(avg_segment_length / 2)  # the number of sentences we will generate per segment\\n\\nmax_sent_tokens = 64\\naugmented_segments = []\\n\\nfor segment in test_segments:\\n    first_sentence = truncate_by_token(segment[0], max_sent_tokens)\\n\\n    augmented_segment = []\\n    for i in range(0, avg_segment_length):\\n        next_sentence = (\\n            segment[0] if len(augmented_segment) == 0 else augmented_segment[-1]\\n        )\\n        next_sentence = \\\" \\\".join(word_tokenize(next_sentence)[:max_sent_tokens])\\n        next_sentence_length = len(next_sentence)\\n        sentence_tokens_length = len(word_tokenize(next_sentence))\\n        # create segment\\n        augmented_sentence = Augmentor.augment_gpt2_single(\\n            next_sentence,\\n            fast=True,\\n            # add the length of the current sentence to the dataset avg length of sentence\\n            output_tokens=int(n * int(dataset_avg_sentence_length)),\\n            num_return_sequences=1,\\n        )\\n\\n        augmented_sentence = post_augmentation_processing(\\n            # feed in the first generated sentence\\n            augmented_sentence[0][0],\\n            next_sentence_length,\\n            n,\\n        )\\n\\n        augmented_segment.append(augmented_sentence)\\n        print(\\\"augmented_segment\\\", augmented_segment)\\n\\n    augmented_segments.append(augmented_segment)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GTA 1 POC\n",
    "dataset_avg_sentence_length = sum(\n",
    "    [avg_segment_length_by_token(segment, floor=True) for segment in test_segments]\n",
    ") // len(test_segments)\n",
    "\n",
    "avg_segment_length = sum([len(t) for t in test_segments]) // len(\n",
    "    test_segments\n",
    ")  # avg number of sentences per segment in the dataset\n",
    "n = int(avg_segment_length / 2)  # the number of sentences we will generate per segment\n",
    "\n",
    "max_sent_tokens = 64\n",
    "augmented_segments = []\n",
    "\n",
    "for segment in test_segments:\n",
    "    first_sentence = truncate_by_token(segment[0], max_sent_tokens)\n",
    "\n",
    "    augmented_segment = []\n",
    "    for i in range(0, avg_segment_length):\n",
    "        next_sentence = (\n",
    "            segment[0] if len(augmented_segment) == 0 else augmented_segment[-1]\n",
    "        )\n",
    "        next_sentence = \" \".join(word_tokenize(next_sentence)[:max_sent_tokens])\n",
    "        next_sentence_length = len(next_sentence)\n",
    "        sentence_tokens_length = len(word_tokenize(next_sentence))\n",
    "        # create segment\n",
    "        augmented_sentence = Augmentor.augment_gpt2_single(\n",
    "            next_sentence,\n",
    "            fast=True,\n",
    "            # add the length of the current sentence to the dataset avg length of sentence\n",
    "            output_tokens=int(n * int(dataset_avg_sentence_length)),\n",
    "            num_return_sequences=1,\n",
    "        )\n",
    "\n",
    "        augmented_sentence = post_augmentation_processing(\n",
    "            # feed in the first generated sentence\n",
    "            augmented_sentence[0][0],\n",
    "            next_sentence_length,\n",
    "            n,\n",
    "        )\n",
    "\n",
    "        augmented_segment.append(augmented_sentence)\n",
    "        print(\"augmented_segment\", augmented_segment)\n",
    "\n",
    "    augmented_segments.append(augmented_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "12127b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 2)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 128;\n",
       "                var nbb_unformatted_code = \"n * int(dataset_avg_sentence_length), n\";\n",
       "                var nbb_formatted_code = \"n * int(dataset_avg_sentence_length), n\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n * int(dataset_avg_sentence_length), n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "db92da22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' of the book: \"The',\n",
       "  ' Book of Mormon, by Joseph',\n",
       "  'Smith.\\n\\n, by',\n",
       "  '. L. Sacks',\n",
       "  ', R. J. P']]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 129;\n",
       "                var nbb_unformatted_code = \"augmented_segments\";\n",
       "                var nbb_formatted_code = \"augmented_segments\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "augmented_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6a1a093a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed augmentation...\n",
      "augmented_segment ['. The whole point of this']\n",
      "completed augmentation...\n",
      "augmented_segment ['. The whole point of this', ' is to give the players a']\n",
      "completed augmentation...\n",
      "augmented_segment ['. The whole point of this', ' is to give the players a', ' chance to win a big game']\n",
      "completed augmentation...\n",
      "augmented_segment ['. The whole point of this', ' is to give the players a', ' chance to win a big game', '.\\n\\nThe only thing']\n",
      "completed augmentation...\n",
      "augmented_segment ['. The whole point of this', ' is to give the players a', ' chance to win a big game', '.\\n\\nThe only thing', ' I can say is that I']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['. The whole point of this',\n",
       "  ' is to give the players a',\n",
       "  ' chance to win a big game',\n",
       "  '.\\n\\nThe only thing',\n",
       "  ' I can say is that I']]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 141;\n",
       "                var nbb_unformatted_code = \"Augmentor.gta1(test_segments)\";\n",
       "                var nbb_formatted_code = \"Augmentor.gta1(test_segments)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Augmentor.gta1(test_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaac744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14d02c0f",
   "metadata": {},
   "source": [
    "## GTA 2\n",
    "\n",
    "- Using GPT-2, we take the first truncated portion of the first sentence in a segment and feed it into the model. The output should be the same size as the first sentence length (for averaging similar segment sizes).\n",
    "- That first outputted sentence becomes the target sentence for the augmented segment.\n",
    "- Using the sentence sentence in the real segment, we repeat the first step. The second sentence in the augmented segment will be the output of the real second sentence fed into GPT-2.\n",
    "- Continuing this process, we will be left with an augmented segment the same exact size as the real segment it’s modeled after with hopefully less variance than GTA 1 toward the end of the segments.\n",
    "\n",
    "Cons:\n",
    "- Possible disjointedness with augmented sentences since they may vary quite a bit from the immediate sentence previously due to relying on an intermediary in-between sentence to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1657b935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed augmentation...\n",
      "sentence this is one sentence augmented_sentence  that is almost impossible to translate\n",
      "completed augmentation...\n",
      "sentence about donald trump augmented_sentence  trumpson, I think.\n",
      "completed augmentation...\n",
      "sentence and pakistan augmented_sentence ) to the US.\n",
      "\n",
      "completed augmentation...\n",
      "sentence i am not a republican augmented_sentence . If I am a democrat\n",
      "completed augmentation...\n",
      "sentence rather I am a democrat augmented_sentence ) and I'm not going\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 134;\n",
       "                var nbb_unformatted_code = \"# GTA 2 POC\\ndataset_avg_sentence_length = sum(\\n    [avg_segment_length_by_token(segment, floor=True) for segment in test_segments]\\n) // len(test_segments)\\n\\navg_segment_length = sum([len(t) for t in test_segments]) // len(\\n    test_segments\\n)  # avg number of sentences per segment in the dataset\\nn = int(avg_segment_length / 2)  # the number of sentences we will generate per segment\\n\\nmax_sent_tokens = 64\\naugmented_segments = []\\n\\nfor segment in test_segments:\\n    augmented_segment = []\\n    for sentence in segment:\\n        sentence = truncate_by_token(sentence, max_sent_tokens)\\n        sentence_length = len(sentence)\\n        sentence_tokens_length = len(word_tokenize(next_sentence))\\n\\n        augmented_sentence = Augmentor.augment_gpt2_single(\\n            sentence,\\n            fast=True,\\n            # add the length of the current sentence to the dataset avg length of sentence\\n            output_tokens=int(n * int(dataset_avg_sentence_length)),\\n            num_return_sequences=1,\\n        )\\n\\n        augmented_sentence = post_augmentation_processing(\\n            # feed in the first generated sentence\\n            augmented_sentence[0][0],\\n            sentence_length,\\n            n,\\n        )\\n\\n        augmented_segment.append(augmented_sentence)\\n        print(\\\"sentence\\\", sentence, \\\"augmented_sentence\\\", augmented_sentence)\\n\\n    augmented_segments.append(augmented_segment)\";\n",
       "                var nbb_formatted_code = \"# GTA 2 POC\\ndataset_avg_sentence_length = sum(\\n    [avg_segment_length_by_token(segment, floor=True) for segment in test_segments]\\n) // len(test_segments)\\n\\navg_segment_length = sum([len(t) for t in test_segments]) // len(\\n    test_segments\\n)  # avg number of sentences per segment in the dataset\\nn = int(avg_segment_length / 2)  # the number of sentences we will generate per segment\\n\\nmax_sent_tokens = 64\\naugmented_segments = []\\n\\nfor segment in test_segments:\\n    augmented_segment = []\\n    for sentence in segment:\\n        sentence = truncate_by_token(sentence, max_sent_tokens)\\n        sentence_length = len(sentence)\\n        sentence_tokens_length = len(word_tokenize(next_sentence))\\n\\n        augmented_sentence = Augmentor.augment_gpt2_single(\\n            sentence,\\n            fast=True,\\n            # add the length of the current sentence to the dataset avg length of sentence\\n            output_tokens=int(n * int(dataset_avg_sentence_length)),\\n            num_return_sequences=1,\\n        )\\n\\n        augmented_sentence = post_augmentation_processing(\\n            # feed in the first generated sentence\\n            augmented_sentence[0][0],\\n            sentence_length,\\n            n,\\n        )\\n\\n        augmented_segment.append(augmented_sentence)\\n        print(\\\"sentence\\\", sentence, \\\"augmented_sentence\\\", augmented_sentence)\\n\\n    augmented_segments.append(augmented_segment)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GTA 2 POC\n",
    "dataset_avg_sentence_length = sum(\n",
    "    [avg_segment_length_by_token(segment, floor=True) for segment in test_segments]\n",
    ") // len(test_segments)\n",
    "\n",
    "avg_segment_length = sum([len(t) for t in test_segments]) // len(\n",
    "    test_segments\n",
    ")  # avg number of sentences per segment in the dataset\n",
    "n = int(avg_segment_length / 2)  # the number of sentences we will generate per segment\n",
    "\n",
    "max_sent_tokens = 64\n",
    "augmented_segments = []\n",
    "\n",
    "for segment in test_segments:\n",
    "    augmented_segment = []\n",
    "    for sentence in segment:\n",
    "        sentence = truncate_by_token(sentence, max_sent_tokens)\n",
    "        sentence_length = len(sentence)\n",
    "        sentence_tokens_length = len(word_tokenize(next_sentence))\n",
    "\n",
    "        augmented_sentence = Augmentor.augment_gpt2_single(\n",
    "            sentence,\n",
    "            fast=True,\n",
    "            # add the length of the current sentence to the dataset avg length of sentence\n",
    "            output_tokens=int(n * int(dataset_avg_sentence_length)),\n",
    "            num_return_sequences=1,\n",
    "        )\n",
    "\n",
    "        augmented_sentence = post_augmentation_processing(\n",
    "            # feed in the first generated sentence\n",
    "            augmented_sentence[0][0],\n",
    "            sentence_length,\n",
    "            n,\n",
    "        )\n",
    "\n",
    "        augmented_segment.append(augmented_sentence)\n",
    "        print(\"sentence\", sentence, \"augmented_sentence\", augmented_sentence)\n",
    "\n",
    "    augmented_segments.append(augmented_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d43bed4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['this is one sentence',\n",
       "   'about donald trump',\n",
       "   'and pakistan',\n",
       "   'i am not a republican',\n",
       "   'rather I am a democrat']],\n",
       " [[' that is almost impossible to translate',\n",
       "   ' trumpson, I think.',\n",
       "   ') to the US.\\n',\n",
       "   '. If I am a democrat',\n",
       "   \") and I'm not going\"]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 135;\n",
       "                var nbb_unformatted_code = \"test_segments, augmented_segments\";\n",
       "                var nbb_formatted_code = \"test_segments, augmented_segments\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_segments, augmented_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15af18ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
